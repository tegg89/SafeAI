{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teggsung/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator import estimator\n",
    "from tensorflow.python.estimator import model_fn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOverall structure:\\n\\n    input: two different dataset\\n        Call dataset operations\\n        densenet: L=200, k=12, dropout=0; ep=300, batch_size=64\\n        wideresnet: depth=28, width=10, dropout=0; ep=200, batch_size=128\\n        \\n    modules: input perturbation\\n             temperature scaling\\n             \\n    op: SGD with Nesterov momentum (0.9)\\n    lr: start=0.1, decay_rate=0.1, decay_time=[50,75]\\n    \\n    in-distribution datasets: CIFAR-10, CIFAR-100\\n    out-of-distribution datasets: TinyImageNet, LSUN, iSUN, Gaussian noise, Uniform noise\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Overall structure:\n",
    "\n",
    "    input: two different dataset\n",
    "        Call dataset operations\n",
    "        densenet: L=200, k=12, dropout=0; ep=300, batch_size=64\n",
    "        wideresnet: depth=28, width=10, dropout=0; ep=200, batch_size=128\n",
    "        \n",
    "    modules: input perturbation\n",
    "             temperature scaling\n",
    "             \n",
    "    op: SGD with Nesterov momentum (0.9)\n",
    "    lr: start=0.1, decay_rate=0.1, decay_time=[50,75]\n",
    "    \n",
    "    in-distribution datasets: CIFAR-10, CIFAR-100\n",
    "    out-of-distribution datasets: TinyImageNet, LSUN, iSUN, Gaussian noise, Uniform noise\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temperature scaling\n",
    "def temp_scaling(x, model, temperature):\n",
    "    scaled = tf.nn.softmax_cross_entropy_with_logits_v2(x, model(x)/temperature)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image preprocessing\n",
    "def preprocess(x, model, eps, temperature):\n",
    "    perturbed = temp_scaling(x, model, temperature)\n",
    "    prep_x = x - eps * np.sign(-perturbed)\n",
    "    return prep_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare score\n",
    "def check_in_out(x, y, model, thresh, temperature=1000, eps=0.0012):\n",
    "    score = tf.nn.softmax_cross_entropy_with_logits_v2(preprocess(x, model, eps, temperature), y)\n",
    "    if score >= thresh:\n",
    "        print('in-distribution')\n",
    "    else:\n",
    "        print('out-of-distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference: https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py\n",
    "## TODO: Need to create method for loadinng pre-trained model?\n",
    "\n",
    "def create_model(data_format):\n",
    "    \"\"\"Model to recognize digits in the MNIST dataset.\n",
    "    Network structure is equivalent to:\n",
    "    https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/examples/tutorials/mnist/mnist_deep.py\n",
    "    and\n",
    "    https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py\n",
    "    But uses the tf.keras API.\n",
    "    Args:\n",
    "    data_format: Either 'channels_first' or 'channels_last'. 'channels_first' is\n",
    "      typically faster on GPUs while 'channels_last' is typically faster on\n",
    "      CPUs. See\n",
    "      https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "    Returns:\n",
    "    A tf.keras.Model.\n",
    "    \"\"\"\n",
    "    if data_format == 'channels_first':\n",
    "        input_shape = [1, 28, 28]\n",
    "    else:\n",
    "        assert data_format == 'channels_last'\n",
    "        input_shape = [28, 28, 1]\n",
    "    \n",
    "    \n",
    "    ## TODO: Transfer TF to keras\n",
    "    '''From Densenet TF official code'''\n",
    "    def conv(image, filters, strides=1, kernel_size=3):\n",
    "    \"\"\"Convolution with default options from the densenet paper.\"\"\"\n",
    "    # Use initialization from https://arxiv.org/pdf/1502.01852.pdf\n",
    "\n",
    "    return tf.layers.conv2d(\n",
    "        inputs=image,\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=tf.identity,\n",
    "        use_bias=False,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=tf.variance_scaling_initializer(),\n",
    "    )\n",
    "\n",
    "    def dense_block(image, filters, is_training):\n",
    "    \"\"\"Standard BN+Relu+conv block for DenseNet.\"\"\"\n",
    "    image = tf.layers.batch_normalization(\n",
    "        inputs=image,\n",
    "        axis=-1,\n",
    "        training=is_training,\n",
    "        fused=True,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        momentum=_BATCH_NORM_DECAY,\n",
    "        epsilon=_BATCH_NORM_EPSILON,\n",
    "    )\n",
    "\n",
    "    if FLAGS.use_bottleneck:\n",
    "        # Add bottleneck layer to optimize computation and reduce HBM space\n",
    "        image = tf.nn.relu(image)\n",
    "        image = conv(image, 4 * filters, strides=1, kernel_size=1)\n",
    "        image = tf.layers.batch_normalization(\n",
    "            inputs=image,\n",
    "            axis=-1,\n",
    "            training=is_training,\n",
    "            fused=True,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            momentum=_BATCH_NORM_DECAY,\n",
    "            epsilon=_BATCH_NORM_EPSILON,\n",
    "        )\n",
    "\n",
    "    image = tf.nn.relu(image)\n",
    "    return conv(image, filters)\n",
    "\n",
    "\n",
    "    def transition_layer(image, filters, is_training):\n",
    "    \"\"\"Construct the transition layer with specified growth rate.\"\"\"\n",
    "\n",
    "    image = tf.layers.batch_normalization(\n",
    "        inputs=image,\n",
    "        axis=-1,\n",
    "        training=is_training,\n",
    "        fused=True,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        momentum=_BATCH_NORM_DECAY,\n",
    "        epsilon=_BATCH_NORM_EPSILON,\n",
    "    )\n",
    "    image = tf.nn.relu(image)\n",
    "    conv_img = conv(image, filters=filters, kernel_size=1)\n",
    "    return tf.layers.average_pooling2d(\n",
    "        conv_img, pool_size=2, strides=2, padding=\"same\")\n",
    "\n",
    "\n",
    "    def _int_shape(layer):\n",
    "        return layer.get_shape().as_list()\n",
    "\n",
    "\n",
    "    # Definition of the CIFAR-10 network\n",
    "    def densenet_cifar_model(image,\n",
    "                             k,\n",
    "                             layers,\n",
    "                             is_training=True,\n",
    "                             num_blocks=3,\n",
    "                             num_classes=10):\n",
    "        \"\"\"Construct a DenseNet with the specified growth size and layers.\"\"\"\n",
    "        layers_per_block = int((layers - 4) / num_blocks)\n",
    "\n",
    "        v = conv(image, filters=2*k, strides=(1, 1), kernel_size=(3, 3))\n",
    "        for i in range(num_blocks):\n",
    "            with tf.variable_scope(\"block-%d\" % i):\n",
    "                for j in range(layers_per_block):\n",
    "                    with tf.variable_scope(\"conv-%d-%d\" % (i, j)):\n",
    "                        dv = dense_block(v, k, is_training)\n",
    "                        v = tf.concat([v, dv], axis=3)\n",
    "            if i != num_blocks - 1:\n",
    "                with tf.variable_scope(\"transition-%d\" % i):\n",
    "                    v = transition_layer(v, _int_shape(v)[3], is_training)\n",
    "\n",
    "        global_pool = tf.reduce_sum(v, axis=(2, 3), name=\"global_pool\")\n",
    "        logits = tf.layers.dense(\n",
    "            global_pool,\n",
    "            units=num_classes,\n",
    "            activation=tf.identity,\n",
    "            kernel_initializer=tf.random_normal_initializer(stddev=2.0 / (\n",
    "                _int_shape(global_pool)[1] * 10)),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\n",
    "            bias_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\n",
    "        )\n",
    "        return logits\n",
    "\n",
    "'''Previous Keras MNIST code'''\n",
    "    l = tf.keras.layers\n",
    "    max_pool = l.MaxPooling2D(\n",
    "        (2, 2), (2, 2), padding='same', data_format=data_format)\n",
    "    # The model consists of a sequential chain of layers, so tf.keras.Sequential\n",
    "    # (a subclass of tf.keras.Model) makes for a compact description.\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            l.Reshape(\n",
    "                target_shape=input_shape,\n",
    "                input_shape=(28 * 28,)),\n",
    "            l.Conv2D(\n",
    "                32,\n",
    "                5,\n",
    "                padding='same',\n",
    "                data_format=data_format,\n",
    "                activation=tf.nn.relu),\n",
    "            max_pool,\n",
    "            l.Conv2D(\n",
    "                64,\n",
    "                5,\n",
    "                padding='same',\n",
    "                data_format=data_format,\n",
    "                activation=tf.nn.relu),\n",
    "            max_pool,\n",
    "            l.Flatten(),\n",
    "            l.Dense(1024, activation=tf.nn.relu),\n",
    "            l.Dropout(0.4),\n",
    "            l.Dense(10)\n",
    "        ])\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"The model_fn argument for creating an Estimator.\"\"\"\n",
    "    model = create_model(params['data_format'])\n",
    "    image = features\n",
    "    if isinstance(image, dict):\n",
    "        image = features['image']\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        logits = model(image, training=False)\n",
    "        predictions = {\n",
    "            'classes': tf.argmax(logits, axis=1),\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=tf.estimator.ModeKeys.PREDICT,\n",
    "            predictions=predictions,\n",
    "            export_outputs={\n",
    "                'classify': tf.estimator.export.PredictOutput(predictions)\n",
    "            })\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "\n",
    "        logits = model(image, training=True)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        accuracy = tf.metrics.accuracy(\n",
    "            labels=labels, predictions=tf.argmax(logits, axis=1))\n",
    "\n",
    "        # Name tensors to be logged with LoggingTensorHook.\n",
    "        tf.identity(LEARNING_RATE, 'learning_rate')\n",
    "        tf.identity(loss, 'cross_entropy')\n",
    "        tf.identity(accuracy[1], name='train_accuracy')\n",
    "\n",
    "        # Save accuracy scalar to Tensorboard output.\n",
    "        tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=tf.estimator.ModeKeys.TRAIN,\n",
    "            loss=loss,\n",
    "            train_op=optimizer.minimize(loss, tf.train.get_or_create_global_step()))\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        logits = model(image, training=False)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=tf.estimator.ModeKeys.EVAL,\n",
    "            loss=loss,\n",
    "            eval_metric_ops={\n",
    "                'accuracy': tf.metrics.accuracy(\n",
    "                                labels=labels, predictions=tf.argmax(logits, axis=1)),\n",
    "            })\n",
    "\n",
    "# def cnn_model_fn(features, labels, mode):\n",
    "#     input_layer = features['images']\n",
    "    \n",
    "#     conv1 = tf.layers.conv2d(\n",
    "#         inputs=input_layer,\n",
    "#         filters=32,\n",
    "#         kernel_size=[5,5],\n",
    "#         padding='same',\n",
    "#         activation=None)\n",
    "#     batch_norm1 = tf.layers.batch_normalization(conv1)\n",
    "#     relu1 = tf.nn.relu(batch_norm1)\n",
    "#     pool1 = tf.layers.max_pooling2d(inputs=relu1, pool_size=[2,2], strides=2)\n",
    "    \n",
    "#     conv2 = tf.layers.conv2d(\n",
    "#         inputs=pool1,\n",
    "#         filters=64,\n",
    "#         kernel_size=[5,5],\n",
    "#         padding='same',\n",
    "#         activation=None)\n",
    "#     batch_norm2 = tf.layers.batch_normalization(conv2)\n",
    "#     relu2 = tf.nn.relu(batch_norm2)\n",
    "#     pool2 = tf.layers.max_pooling2d(inputs=relu2, pool_size=[2,2], strides=2)\n",
    "    \n",
    "#     pool2_flat = tf.layers.flatten(pool2)\n",
    "    \n",
    "#     dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    \n",
    "#     dropout = tf.layers.dropout(inputs=dense, rate=0.4, \n",
    "#                                 training={mode == tf.estimator.ModeKeys.TRAIN})\n",
    "    \n",
    "#     logits = tf.layers.dense(inputs=dropout, units=params['num_classes'])\n",
    "    \n",
    "#     predictions = {'classes': tf.argmax(input=logits, axis=1),\n",
    "#                    'probabilities': tf.nn.softmax(logits, name='softmax_tensor')}\n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "#         spec = tf.estimator.EstimatorSpec(\n",
    "#             mode=mode, predictions=predictions, \n",
    "#             export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)})\n",
    "#         return spec\n",
    "        \n",
    "#     loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#         spec = tf.estimator.EstimatorSpec(\n",
    "#             optimizer = tf.train.AdamOptimizer(),\n",
    "#             train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step()) )\n",
    "#         return spec\n",
    "        \n",
    "#     eval_metric_ops = {\n",
    "#         'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions['classes']),\n",
    "#         'aupr': tf.metrics.average_precision_at_k(labels, predictions, k=k)\n",
    "#     }\n",
    "#     spec = tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "#     return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-909a226d874c>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-909a226d874c>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    op =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def odin_classifier(dataset, model, mode, params):\n",
    "#     labels = tf.cast(labels, tf.int32)\n",
    "    \n",
    "    ## Check available mode\n",
    "    if mode not in [model_fn.ModeKeys.TRAIN, model_fn.ModeKeys.EVAL, model_fn.ModeKeys.PREDICT]:\n",
    "        raise ValueError('Mode not recognized: {}'.format(mode))\n",
    "        \n",
    "#    assert isinstance() or isinstance()\n",
    "    \n",
    "    check_in(x=dataset1,\n",
    "             y=dataset2,\n",
    "             model=model,\n",
    "             thresh=0.5)\n",
    "    \n",
    "    # Define training operations\n",
    "    op =\n",
    "    loss = \n",
    "    \n",
    "    # Define accuracy, metrics\n",
    "    metrics = \n",
    "    accuracy = \n",
    "    \n",
    "    \n",
    "    # Eval\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "        merged = tf.summary.merge_all()\n",
    "        test_writer = tf.summary.FileWriter('test')\n",
    "        return spec\n",
    "\n",
    "    # Train: Alternative learning\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, train_op=op)\n",
    "        tf.summary.scalar('accuracy', accuracy[1])\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('train')\n",
    "        return spec\n",
    "\n",
    "    raise ValueError(\"Invalid estimator mode: reached the end of the function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff4e6ca203ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpredictionns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "## Metric\n",
    "tf.metrics.auc(labels,  predictionns)\n",
    "tf.metrics.accuracy(labels, predictions)\n",
    "tf.metrics.precision(labels, predictions)\n",
    "tf.metrics.recall(labels, predictions)\n",
    "tf.metrics.average_precision_at_k(labels, predictions, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'steps': 10000,\n",
    "    'num_classes': 10,\n",
    "    'model_dir': './ckpt/',\n",
    "    'saved_dir': '/pb/',\n",
    "    'learning_rate': 1e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dataset1 = ...\\n    dataset2 = ...\\n    odin_clasifier(dataset1, dataset2)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' dataset1 = ...\n",
    "    dataset2 = ...\n",
    "    odin_clasifier(dataset1, dataset2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './ckpt/', '_tf_random_seed': None, '_save_summary_steps': 20, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb20e46ef0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "training_config = tf.estimator.RunConfig(\n",
    "    model_dir=params['model_dir'],\n",
    "    save_summary_steps=20,\n",
    "    save_checkpoints_steps=20)\n",
    "classifier = tf.estimator.Estimator(\n",
    "    config=training_config,\n",
    "    model_fn=cnn_model_fn,\n",
    "    model_dir=params['model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_to_log = {'probabilities': 'softmax_tensors'}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6be87a9b35da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_input_fn = tf.estimator.inputs.numpy_input_fn(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=params['steps'],\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "classifier.train(input_fn=train_input_fn, \n",
    "                 steps=params['steps'], \n",
    "                 hooks=[logging_hook])\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'image': eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
