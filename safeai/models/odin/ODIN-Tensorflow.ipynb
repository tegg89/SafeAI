{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teggsung/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator import estimator\n",
    "from tensorflow.python.estimator import model_fn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO:\\n    input: two different dataset\\n        Call dataset operations\\n        densenet: L=200, k=12, dropout=0; ep=300, batch_size=64\\n        wideresnet: depth=28, width=10, dropout=0; ep=200, batch_size=128\\n        \\n    modules: input perturbation\\n             temperature scaling\\n             \\n    op: SGD with Nesterov momentum (0.9)\\n    lr: start=0.1, decay_rate=0.1, decay_time=[50,75]\\n    \\n    in-distribution datasets: CIFAR-10, CIFAR-100\\n    out-of-distribution datasets: TinyImageNet, LSUN, iSUN, Gaussian noise, Uniform noise'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO:\n",
    "    input: two different dataset\n",
    "        Call dataset operations\n",
    "        densenet: L=200, k=12, dropout=0; ep=300, batch_size=64\n",
    "        wideresnet: depth=28, width=10, dropout=0; ep=200, batch_size=128\n",
    "        \n",
    "    modules: input perturbation\n",
    "             temperature scaling\n",
    "             \n",
    "    op: SGD with Nesterov momentum (0.9)\n",
    "    lr: start=0.1, decay_rate=0.1, decay_time=[50,75]\n",
    "    \n",
    "    in-distribution datasets: CIFAR-10, CIFAR-100\n",
    "    out-of-distribution datasets: TinyImageNet, LSUN, iSUN, Gaussian noise, Uniform noise'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO: Load actual datasets & preprocess to get ready'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO: Load actual datasets & preprocess to get ready'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temperature scaling\n",
    "def temp_scaling(x, model, temperature):\n",
    "    scaled = tf.nn.softmax_cross_entropy_with_logits_v2(x, model(x)/temperature)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image preprocessing\n",
    "def preprocess(x, model, eps, temperature):\n",
    "    perturbed = temp_scaling(x, model, temperature)\n",
    "    prep_x = x - eps * np.sign(-perturbed)\n",
    "    return prep_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare score\n",
    "def check_in_out(x, y, model, thresh, temperature=1000, eps=0.0012):\n",
    "    score = tf.nn.softmax_cross_entropy_with_logits_v2(preprocess(x, model, eps, temperature), y)\n",
    "    if score >= thresh:\n",
    "        print('in-distribution')\n",
    "    else:\n",
    "        print('out-of-distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-ef702dcd7085>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-ef702dcd7085>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    op =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def odin_classifier(dataset, model, mode, params):\n",
    "#     labels = tf.cast(labels, tf.int32)\n",
    "    \n",
    "    ## Check available mode\n",
    "    if mode not in [model_fn.ModeKeys.TRAIN, model_fn.ModeKeys.EVAL, model_fn.ModeKeys.PREDICT]:\n",
    "        raise ValueError('Mode not recognized: {}'.format(mode))\n",
    "        \n",
    "#    assert isinstance() or isinstance()\n",
    "    \n",
    "    check_in(x=dataset1,\n",
    "             y=dataset2,\n",
    "             model=model,\n",
    "             thresh=0.5)\n",
    "    \n",
    "    # Define training operations\n",
    "    op =\n",
    "    loss = \n",
    "    \n",
    "    # Define accuracy, metrics\n",
    "    metrics = \n",
    "    accuracy = \n",
    "    \n",
    "    \n",
    "    # Eval\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "        merged = tf.summary.merge_all()\n",
    "        test_writer = tf.summary.FileWriter('test')\n",
    "        return spec\n",
    "\n",
    "    # Train: Alternative learning\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, train_op=op)\n",
    "        tf.summary.scalar('accuracy', accuracy[1])\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('train')\n",
    "        return spec\n",
    "\n",
    "    raise ValueError(\"Invalid estimator mode: reached the end of the function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff4e6ca203ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpredictionns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "## Metric\n",
    "tf.metrics.auc(labels,  predictionns)\n",
    "tf.metrics.accuracy(labels, predictions)\n",
    "tf.metrics.precision(labels, predictions)\n",
    "tf.metrics.recall(labels, predictions)\n",
    "tf.metrics.average_precision_at_k(labels, predictions, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    input_layer = features['images']\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5,5],\n",
    "        padding='same',\n",
    "        activation=None)\n",
    "    batch_norm1 = tf.layers.batch_normalization(conv1)\n",
    "    relu1 = tf.nn.relu(batch_norm1)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=relu1, pool_size=[2,2], strides=2)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5,5],\n",
    "        padding='same',\n",
    "        activation=None)\n",
    "    batch_norm2 = tf.layers.batch_normalization(conv2)\n",
    "    relu2 = tf.nn.relu(batch_norm2)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=relu2, pool_size=[2,2], strides=2)\n",
    "    \n",
    "    pool2_flat = tf.layers.flatten(pool2)\n",
    "    \n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    \n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, \n",
    "                                training={mode == tf.estimator.ModeKeys.TRAIN})\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=dropout, units=params['num_classes'])\n",
    "    \n",
    "    predictions = {'classes': tf.argmax(input=logits, axis=1),\n",
    "                   'probabilities': tf.nn.softmax(logits, name='softmax_tensor')}\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode, predictions=predictions, \n",
    "            export_outputs={'predict': tf.estimator.export.PredictOutput(predictions)})\n",
    "        return spec\n",
    "        \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            optimizer = tf.train.AdamOptimizer(),\n",
    "            train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step()) )\n",
    "        return spec\n",
    "        \n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions['classes']),\n",
    "        'aupr': tf.metrics.average_precision_at_k(labels, predictions, k=k)\n",
    "    }\n",
    "    spec = tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndataset1 = ...\\ndataset2 = ...\\nodin_clasifier(dataset1, dataset2)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dataset1 = ...\n",
    "dataset2 = ...\n",
    "odin_clasifier(dataset1, dataset2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'steps': 10000,\n",
    "    'num_classes': 10,\n",
    "    'model_dir': './ckpt/',\n",
    "    'saved_dir': '/pb/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './ckpt/', '_tf_random_seed': None, '_save_summary_steps': 20, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb20e46ef0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "training_config = tf.estimator.RunConfig(\n",
    "    model_dir=params['model_dir'],\n",
    "    save_summary_steps=20,\n",
    "    save_checkpoints_steps=20)\n",
    "classifier = tf.estimator.Estimator(\n",
    "    config=training_config,\n",
    "    model_fn=cnn_model_fn,\n",
    "    model_dir=params['model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_to_log = {'probabilities': 'softmax_tensors'}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6be87a9b35da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_input_fn = tf.estimator.inputs.numpy_input_fn(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=params['steps'],\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "classifier.train(input_fn=train_input_fn, \n",
    "                 steps=params['steps'], \n",
    "                 hooks=[logging_hook])\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'image': eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
